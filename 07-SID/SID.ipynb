{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7cf67c-472d-40d4-9b3d-caf6513248eb",
   "metadata": {},
   "source": [
    "![DLI Logo](../images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d79f8-8a3b-4632-b816-e385246f5cf5",
   "metadata": {},
   "source": [
    "# Sensitive Information Detection with Morpheus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac617bc-5bdb-4364-a159-f9dbfae27d6e",
   "metadata": {},
   "source": [
    "In this notebook, you will begin working with the NLP pipeline to perform sensitive information detection (SID) on packet capture data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841f392-5f45-405e-99f6-3a1e8c61b0fb",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc237671-e2a0-471b-94c0-2f066cfcd155",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "\n",
    "- Be familiar with the key features specific to the Morpheus NLP pipeline.\n",
    "- Be able to perform sensitive information detection using the Morpheus NLP pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4677e025-92ed-4274-a75b-2e64bcd33f7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f918c-2992-4910-ac2c-1dab2fa86a3e",
   "metadata": {},
   "source": [
    "## The Morpheus NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcbc376-a0bb-4386-b4ce-3c840643a346",
   "metadata": {},
   "source": [
    "Morpheus offers `pipeline-nlp` and ships with an already-trained BERT-based model that can identify several kinds of sensitive data. We will now turn our attention to utilizing `pipeline-nlp` to perform sensitive information detection (SID) on a packet capture dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1dd37-e32b-4bc7-919a-d579bd24ac66",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92cde28-7630-4bcd-86ad-50bd41b3c03a",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce15bc-3035-4e8c-ae07-ef395f5d3737",
   "metadata": {},
   "source": [
    "The source data for our pipeline will be `data/pcap_dump.jsonlines`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2fbb34-10cb-4e2c-92b4-7e7706d8997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-rw- 1 root root 45M Mar 19  2022 data/pcap_dump.jsonlines\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/pcap_dump.jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e5d7b-d440-4d80-a9ef-5e96ed32afae",
   "metadata": {},
   "source": [
    "`pcap_dump.jsonlines` contains 93085 packet captures, each represented as a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abc60d5-fb69-4412-8caa-e433fcbb65a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93085\n"
     ]
    }
   ],
   "source": [
    "!cat data/pcap_dump.jsonlines | wc -l # Count number of lines / packet captures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b869d-b125-4ec8-800a-70648e7df8fc",
   "metadata": {},
   "source": [
    "We will be using the [`jq`](https://stedolan.github.io/jq/) library to help us read both this input JSON data, and later the output data. We could also easily use a dataframe, but the `jq` output will be more legible.\n",
    "\n",
    "Run the following cell to look at 2 arbitrary packet captures from the input data, paying special attention to the `data` fields, which might include sensitive information we would like know is being sent through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de9b490-2923-494c-854b-7949a8f5fc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"timestamp\": 1616380971991,\n",
      "  \"host_ip\": \"10.188.40.56\",\n",
      "  \"data_len\": \"139\",\n",
      "  \"data\": \"\"{\"markerEmail\": \"FuRLFaAZ identify benefit BneiMvCZ join 92694759\"}\"\",\n",
      "  \"src_mac\": \"04:3f:72:bf:af:74\",\n",
      "  \"dest_mac\": \"b4:a9:fc:3c:46:f8\",\n",
      "  \"protocol\": \"6\",\n",
      "  \"src_ip\": \"10.244.0.1\",\n",
      "  \"dest_ip\": \"10.244.0.25\",\n",
      "  \"src_port\": \"50410\",\n",
      "  \"dest_port\": \"80\",\n",
      "  \"flags\": \"24\",\n",
      "  \"is_pii\": false\n",
      "}\n",
      "{\n",
      "  \"timestamp\": 1616380972831,\n",
      "  \"host_ip\": \"10.188.40.56\",\n",
      "  \"data_len\": \"310\",\n",
      "  \"data\": \"POST /simpledatagen/ HTTP/1.1rnHost: echo.gtc1.netqdev.cumulusnetworks.comrnUser-Agent: python-requests/2.22.0rnAccept-Encoding: gzip, deflaternAccept: */*rnConnection: keep-alivernContent-Length: 434rnContent-Type: application/jsonrnrn\",\n",
      "  \"src_mac\": \"04:3f:72:bf:af:74\",\n",
      "  \"dest_mac\": \"b4:a9:fc:3c:46:f8\",\n",
      "  \"protocol\": \"6\",\n",
      "  \"src_ip\": \"10.244.0.60\",\n",
      "  \"dest_ip\": \"10.20.16.248\",\n",
      "  \"src_port\": \"50436\",\n",
      "  \"dest_port\": \"80\",\n",
      "  \"flags\": \"24\",\n",
      "  \"is_pii\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Look at 2 arbitrary packet captures at indices `1`, `31`.\n",
    "!cat data/pcap_dump.jsonlines | jq -s '.[1,31]' | tr -d '\\\\' # Remove backslashes for easier reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8874f43-5955-45d3-bcc2-dd8c07a540e8",
   "metadata": {},
   "source": [
    "## Pipeline Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fda12-9c51-4396-9ab5-63c5e5c89150",
   "metadata": {},
   "source": [
    "In order to perform SID on this data we are going to utilize the following `pipeline-nlp`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a367b-c0e4-4a35-9e27-6ad91481e712",
   "metadata": {},
   "source": [
    "```sh\n",
    "morpheus run \\\n",
    "  pipeline-nlp \\\n",
    "    --labels_file=data/labels_nlp.txt \\\n",
    "  from-file --filename=data/pcap_dump.jsonlines \\\n",
    "  deserialize \\\n",
    "  preprocess \\\n",
    "    --vocab_hash_file=data/bert-base-uncased-hash.txt \\\n",
    "    --truncation=True \\\n",
    "    --do_lower_case=True \\\n",
    "  inf-triton \\\n",
    "    --model_name=sid-minibert-onnx \\\n",
    "    --server_url=triton:8001 \\\n",
    "  add-class \\\n",
    "  serialize \\\n",
    "  to-file --filename=data/output/output.jsonlines --overwrite\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1af710-5395-492a-8868-d9c62d3c9bea",
   "metadata": {},
   "source": [
    "Much of what you see in the pipeline will be familiar to you from your earlier work, but there are several differences from the FIL pipeline to note, which we will now turn our attention to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40cdde-364d-4d17-8ead-86cd8efbe9fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9b4b9-55e5-46c4-a9a6-3578d62d3cfb",
   "metadata": {},
   "source": [
    "## Labels File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9caaf5a-ae66-44a0-ab96-f2b9fa59fb52",
   "metadata": {},
   "source": [
    "The NLP pipeline expects a labels file listing the classes that the NLP model in use has been trained to identify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f78d5d7-5cbf-47de-a71d-2846f5f6c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --labels_file FILE              Specifies a file to read labels from in\n",
      "                                  order to convert class IDs into labels. A\n",
      "                                  label file is a simple text file where each\n",
      "                                  line corresponds to a label  [default:\n",
      "                                  data/labels_nlp.txt]\n"
     ]
    }
   ],
   "source": [
    "!morpheus run pipeline-nlp --help | grep 'labels_file' -A 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612e675-5350-4463-8c8d-46af6a68340f",
   "metadata": {},
   "source": [
    "For our pipeline we have stored the labels file in its expected default location `data/labels_nlp.txt`, and it contains the kinds of sensitive information our model has been trained to detect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da0dd97-329e-4d29-a213-4c09f0bce71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address\n",
      "bank_acct\n",
      "credit_card\n",
      "email\n",
      "govt_id\n",
      "name\n",
      "password\n",
      "phone_num\n",
      "secret_keys\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "!cat data/labels_nlp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6177c0-28c7-4354-b7f1-1789c135a1b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7141b-4f74-487e-8677-73458bdb2e3a",
   "metadata": {},
   "source": [
    "## NLP Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4526a0-03ad-433c-ab1f-ae498e2c8393",
   "metadata": {},
   "source": [
    "Given that each kind of Morpheus pipeline performs inference with a different kind of model, it makes sense that for each kind of pipeline that the actions performed by the `preprocessing` stage are distinct. In the case of `pipeline-nlp` there are several options available to us that are specific to the NLP model we will perform inference with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1db24d-a237-4e81-9b33-86dbd8af3dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mConfiguring Pipeline via CLI\u001b[0m\n",
      "Usage: morpheus run pipeline-nlp preprocess [OPTIONS]\n",
      "\n",
      "Options:\n",
      "  --vocab_hash_file FILE        Path to hash file containing vocabulary of\n",
      "                                words with token-ids. This can be created from\n",
      "                                the raw vocabulary using the\n",
      "                                cudf.utils.hash_vocab_utils.hash_vocab\n",
      "                                function. Default value is 'data/bert-base-\n",
      "                                cased-hash.txt'  [default: data/bert-base-\n",
      "                                cased-hash.txt]\n",
      "  --truncation BOOLEAN          When set to True, any tokens extending past\n",
      "                                the max sequence length will be truncated.\n",
      "                                [default: False]\n",
      "  --do_lower_case BOOLEAN       Converts all strings to lowercase.  [default:\n",
      "                                False]\n",
      "  --add_special_tokens BOOLEAN  Adds special tokens '[CLS]' to the beginning\n",
      "                                and '[SEP]' to the end of each string. .\n",
      "                                [default: False]\n",
      "  --stride INTEGER              If a string extends beyond max sequence\n",
      "                                length, it will be broken up into multiple\n",
      "                                sections. This option specifies how far each\n",
      "                                to increment the head of each string when\n",
      "                                broken into multiple segments. Lower numbers\n",
      "                                will reult in more overlap. Setting this to -1\n",
      "                                will auto calculate the stride to be 75% of\n",
      "                                the max sequence length. If truncation=True,\n",
      "                                this option has no effect.  [default: -1]\n",
      "  --help                        Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!morpheus run pipeline-nlp preprocess --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b43870-d5a7-402c-8d5e-33bcdd02a886",
   "metadata": {},
   "source": [
    "For our pipeline we will provide a vocabulary hash file which contains a mapping of words with token IDs, will truncate any tokens extending past a maximum sequence length, and will convert all incoming strings to lowercase:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8024e-ebee-40bc-a3e7-ea37a0209ab3",
   "metadata": {},
   "source": [
    "```sh\n",
    "preprocess \\\n",
    "--vocab_hash_file=data/bert-base-uncased-hash.txt \\\n",
    "--truncation=True \\\n",
    "--do_lower_case=True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6990af88-a278-492a-bd9d-2d0efaff7fae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa26c0d-f4de-45d5-8388-930ea44c1960",
   "metadata": {},
   "source": [
    "## SID MiniBERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70560c07-7306-4126-a3c5-8e24221c4dbc",
   "metadata": {},
   "source": [
    "For inference we will be utilizing a model trained on top of MiniBERT to perform detection of data matching the labels we viewed above. This model, which ships along with Morpheus, is called `sid-minibert-onnx`, and has already been loaded into Triton for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6bea39-ca6d-4d66-9201-05c309f8de15",
   "metadata": {},
   "source": [
    "```sh\n",
    "inf-triton \\\n",
    "  --model_name=sid-minibert-onnx \\\n",
    "  --server_url=triton:8001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3eb4b0d-3c18-4cbd-88a4-faeadfd93315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m[\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"abp-nvsmi-xgb\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"state\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"READY\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"phishing-bert-onnx\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"state\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"READY\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"sid-minibert-onnx\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"state\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"READY\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!curl -s -X POST triton:8000/v2/repository/index | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b384d1-dbb3-4645-b844-b3fd9492ce08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e761a9f-bab9-4e26-8da2-fde80cf8eff4",
   "metadata": {},
   "source": [
    "## Run the SID Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103bd07e-556d-48f4-b476-8ed9aba133f1",
   "metadata": {},
   "source": [
    "With all those details in place, we are ready to execute the following SID pipeline.\n",
    "\n",
    "Compared to the FIL pipeline we have run in previous sections, this pipeline processes a lot more data and will take a significantly longer time to complete. With that in mind, we will run the pipeline in a Jupyter terminal, so we can investigate its output here in the notebook while it continues to run.\n",
    "\n",
    "- Open a Jupyter terminal\n",
    "- `cd` into `/dli/task/07-SID`\n",
    "- Do `./launch_sid.sh` which contains the pipeline we have been describing along with a couple `monitor` stages to monitor throughput for the preprocessing and inference stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3206eb-d45c-41d1-8bdb-b5cbb8d11d19",
   "metadata": {},
   "source": [
    "You should see `Starting pipeline via CLI...` and shortly thereafter info about the preprocessing and inference rates for the pipeline:\n",
    "\n",
    "```\n",
    "Preprocessing rate: 93085messages [01:02, 1488.33messages/s]\n",
    "Inference rate: 66560inf [01:02, 1070.50inf/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaed2d3-c803-4037-ac76-e453f222b4ce",
   "metadata": {},
   "source": [
    "If you like, you can view the exact contents of `launch_sid.sh` by executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad8cdc8c-8667-433c-898e-f2f606b0370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mConfiguring Pipeline via CLI\u001b[0m\n",
      "\u001b[31mStarting pipeline via CLI... Ctrl+C to Quit\u001b[0m\n",
      "Preprocessing rate: 0messages [00:00, ?messages/s]\n",
      "Inference rate: 0inf [00:00, ?inf/s]\u001b[A\n",
      "Preprocessing rate: 256messages [00:00, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:02, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:03, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:04, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:05, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:06, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:07, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:08, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:09, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:10, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:11, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:12, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:13, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:14, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:15, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:16, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:17, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:18, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:19, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:20, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:21, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:22, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:23, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:24, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:25, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:26, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:27, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:28, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:29, 544.14messages/s]\n",
      "Inference rate: 0inf [00:29, ?inf/s]\u001b[A^C\n",
      "                                                           \n",
      "\u001b[AStopping pipeline. Please wait... Press Ctrl+C again to kill.\n",
      "Preprocessing rate: 33280messages [00:29, 544.14messages/s]\n",
      "Preprocessing rate: 33280messages [00:29, 1146.76messages/s]\n",
      "Inference rate: 0inf [00:29, ?inf/s]\n"
     ]
    }
   ],
   "source": [
    "!./launch_sid.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cce11-2c3c-463c-880f-90ad6dc80ada",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997522f1-4097-485a-b7d0-22f4d1661169",
   "metadata": {},
   "source": [
    "## Results Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11506cda-8c6a-4181-b6cb-41bd748ed363",
   "metadata": {},
   "source": [
    "Now that the pipeline is running, we can confirm that it is writing its results to `data/output/output.jsonlines` as we specified in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd08e62-d6aa-4c01-b138-81e86e541499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.jsonlines\n"
     ]
    }
   ],
   "source": [
    "!ls data/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9c830-5a89-4550-bfd1-29711e3c5eec",
   "metadata": {},
   "source": [
    "If you don't see `output.jsonlines` in the output above, wait a few seconds for the pipeline to spin up, and run the cell again until you can confirm it is present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773bff8a-4dde-455b-87ec-13f8fb85d76e",
   "metadata": {},
   "source": [
    "Running the cell below will show that the output file is being continuously written to by the active pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b53f4cc-730d-4268-908e-d4098ab09fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the output file several times for 10 seconds.\n",
    "!for i in {1..5}; do cat data/output/output.jsonlines | wc -l; sleep 2; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ade1b-2ce3-4331-9d6c-b9671ef155ec",
   "metadata": {},
   "source": [
    "Examining the output we can see evidence that the pipeline has correctly labeled packets containing the classes of sensitive information we passed into the pipeline. For example, note the `\"user\"` field for the following entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa0a1c5-208e-4f06-8896-73c412e0fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n"
     ]
    }
   ],
   "source": [
    "!cat data/output/output.jsonlines | jq -s '.[100]' | tr -d '\\\\' # View a packet capture annotated with SID labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd43e03-9b45-4867-a5c8-9e8bc8496038",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce7ed3-b079-41c4-bb6a-7e49d7fcfada",
   "metadata": {},
   "source": [
    "## Sample Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507491a0-2901-402a-947b-f6f60679b2fd",
   "metadata": {},
   "source": [
    "Here is a small collection of sample outputs highlighting some examples of our pipeline's results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07ccfa2-48af-45b8-a7ab-4a0ee859a90d",
   "metadata": {},
   "source": [
    "### Secret Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e626643-17de-4ded-b8e8-04445d1d0e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Show the first 3 data fields for packets identified as having secret keys.\n",
    "!cat data/output/output.jsonlines | \\\n",
    "  jq -s 'map(select(.secret_keys == 1) | {data: .data, secret_key: .secret_keys})[0:3]' | \\\n",
    "  `# Remove backslashes for easier reading.` \\\n",
    "  tr -d '\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1231110-d4db-43ae-ba61-f495e522fa54",
   "metadata": {},
   "source": [
    "### Email and Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e065c43-4622-48b3-a779-692df31f1659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Show the first 3 data fields for packets identified as having both email addresses and passwords.\n",
    "!cat data/output/output.jsonlines | \\\n",
    "  jq -s 'map(select(.email == 1 and .password == 1) | {data: .data, email: .email, password: .password})[0:3]' | \\\n",
    "  `# Remove backslashes for easier reading.` \\\n",
    "  tr -d '\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ae13b-089d-4207-86f3-fb6da377a34f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488c714-b3f0-4341-80d2-a159e25e3f2b",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727a141-1f92-43fb-8486-a5e5b7626cd1",
   "metadata": {},
   "source": [
    "In the next section you will apply what you've learned about the NLP pipeline to construct one of your own.\n",
    "\n",
    "Please continue to the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
