{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b9e4c3-32dc-433c-8326-5766ec3da5b4",
   "metadata": {},
   "source": [
    "![DLI Header](../images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2339b4-efe7-4b7f-be86-aa9b62d3e832",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4253c-0903-4dc4-bcff-7b61603cd3ee",
   "metadata": {},
   "source": [
    "[NVIDIA Tritonâ„¢ Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) delivers fast and scalable AI in production by deploying AI/DL models in a variety of formats. Morpheus provides access to Triton as a back end for performing inference in Morpheus pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4d7cd-b303-42b7-9ffc-d8dbff3a4c3d",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead89244-719a-4e6b-9308-468e2f028cdf",
   "metadata": {},
   "source": [
    "By the end of this notebook you will:\n",
    "\n",
    "- Have a high-level understanding of Triton.\n",
    "- Confirm that Triton is ready for use in this interactive environment.\n",
    "- Know where to find out more about using Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d6994-8383-4634-96a4-21fb8be4db29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84e8c0-7191-4c68-92f7-d17b00e8c8c4",
   "metadata": {},
   "source": [
    "## Triton Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7e2b4-c792-4843-9e52-4299610403aa",
   "metadata": {},
   "source": [
    "Before proceeding, check out this [2-minute introductory video on Triton](https://youtu.be/1kOaYiNVgFs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27466f2-065d-4c06-9e0c-ebbd08e3c6f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a91af-a310-48f0-8b22-8b502f657b46",
   "metadata": {},
   "source": [
    "## Confirm Triton is Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0aad4-1a00-4d62-902f-5af65df80974",
   "metadata": {},
   "source": [
    "As part of this interactive environment, a Triton inference server has already been prepared for use. Triton is running at a host named `triton`. Run the cell below to send an HTTP request to the Triton server to confirm it is up and running. A status code of `200 OK` indicates that it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ef6cd1-ceaa-4b0b-a57b-80885476091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mContent-Length\u001b[0m: 0\n",
      "\u001b[1mContent-Type\u001b[0m: text/plain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl -i triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a97ef-0f7e-4932-9f68-fb991b48711f",
   "metadata": {},
   "source": [
    "## Confirm Model Repositories are Loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a015c89-1a0e-4f3f-98b9-60ade739ca48",
   "metadata": {},
   "source": [
    "When using Triton, we load trained models as \"repositories\" into the server. We can send an HTTP POST request to the Triton server to inspect the models that are already loaded for us to begin using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3eb4b0d-3c18-4cbd-88a4-faeadfd93315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m[\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"abp-nvsmi-xgb\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"state\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"READY\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"phishing-bert-onnx\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"state\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"READY\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"sid-minibert-onnx\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"version\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"state\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"READY\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!curl -s -X POST triton:8000/v2/repository/index | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54641212-5546-49b1-804c-70b47c28c1d5",
   "metadata": {},
   "source": [
    "In the next section we will utilize the `abp-nvsmi-xgb` model. This model ships with the Morpheus toolkit. It is an XGBoost model trained to perform anomalous behavior profiling on GPU-activity logs. Given that the FIL pipeline is designed to perform inference on tree-based models, like XGBoost, this makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf0b6d-5cc3-4b75-a820-05f224333788",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51123c-4b12-432b-9f65-4549db307c82",
   "metadata": {},
   "source": [
    "## Additional Triton Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3354d0-a564-4b4a-918b-c2010060e35a",
   "metadata": {},
   "source": [
    "Triton ships as a part of the Morpheus toolkit, however, a deep dive into Triton is beyond the scope of this workshop. For the many of you who may wish to learn more about Triton, please bookmark the following resources for further study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4459b-e8ad-4d02-afa5-e437f687318d",
   "metadata": {},
   "source": [
    "- [Deploying a Model for Inference at Production Scale](https://courses.nvidia.com/courses/course-v1:DLI+S-FX-03+V1/about): This interactive self-paced DLI course is the best way to start learning Triton.\n",
    "- [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server): The main homepage for Triton.\n",
    "- [NVIDIA Developer Blog](https://developer.nvidia.com/blog/tag/triton/): The NVIDIA Developer Blog has a wealth of resources to help you get started with Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecde6c2-603a-48ee-834c-6c1d827143a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b376d3-2c1c-4cfc-90dc-774b2c12b578",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db51b255-ff03-4a39-9a84-1ad0132d0319",
   "metadata": {},
   "source": [
    "Now that you have a high-level understanding that Triton can host models and perform very fast inference, and that it is enabled in this environment for us to use, it's time to utilize it in a Morpheus pipeline.\n",
    "\n",
    "Please continue to the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
